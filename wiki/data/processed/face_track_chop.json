{
  "id": "face_track_chop",
  "name": "Face Track CHOP",
  "displayName": "Face Track CHOP",
  "category": "CHOP",
  "subcategory": "Generators",
  "version": "",
  "lastUpdated": "2025-08-07T07:50:00.597Z",
  "sourceFile": "C:\\Program Files\\Derivative\\TouchDesigner\\Samples\\Learn\\OfflineHelp\\https.docs.derivative.ca\\Face_Track_CHOP.htm",
  "url": "",
  "description": "OS: This operator is only supported under the Microsoft Windows operating system.Hardware: This operator uses the Augmented Reality (AR) SDK of the Nvidia Maxine system and requires a 20 series and above Nvidia RTX card to operate.",
  "summary": "OS: This operator is only supported under the Microsoft Windows operating system.Hardware: This operator uses the Augmented Reality (AR) SDK of the Nvidia Maxine system and requires a 20 series and ab",
  "details": "",
  "usage": "",
  "tips": [],
  "warnings": [],
  "parameters": [
    {
      "id": null,
      "name": "Active",
      "label": "Active",
      "group": "General",
      "page": "",
      "type": "float",
      "dataType": "number",
      "style": "",
      "defaultValue": null,
      "minValue": null,
      "maxValue": null,
      "step": null,
      "menuItems": [],
      "menuLabels": [],
      "allowCustom": false,
      "maxLength": null,
      "pattern": null,
      "isArray": false,
      "arraySize": 1,
      "dimensions": 1,
      "description": "From Derivative\n\t\t\n\t\t\n\t\t\n\t\t\n\t\tJump to navigation\n\t\tJump to search\n\t\t\nNOTE\n\nOS: This operator is only supported under the Microsoft Windows operating system.Hardware: This operator uses the Augmented Reality (AR) SDK of the Nvidia Maxine system and requires a 20 series and above Nvidia RTX card to operate.\n\n\n\nThe Face Track  can detect faces and facial landmark points in an image, as well as the direction the face is looking relative to the camera. Using a compatible 3D Morphable Face Model (3DMM) and the Face Track SOP, it can also be used to fit and animate a 3D mesh to the detected face.\nThe input image is taken from a provided  and can be of any resolution or format, and either a still image or video. If multiple faces are present in an image, the  will attempt to track the largest one detected.\nThe coordinates of the detected features are given in u, v positions relative to the bottom-left corner of the input image. By default, the values range from 0 to 1, but the 'Aspect Correct' parameter can be enabled to scale the values so that they can be used as 3D coordinates while maintaining the aspect ratio of the original image.\nTip: Look at the several examples of the Face Track / in OP Snippets.\nTo align a 3D rendering of the points with the original input image, set the 'Projection' of your Camera COMP to 'Orthographic', the 'Ortho Origin' parameter to 'Bottom-Left', and the 'Ortho Width' to 1, while also enabling 'Aspect Correct' on the Face Track .\nTo use the mesh fitting features you will need a compatible face mesh file in the Nvidia 'nvf' format. We recommend using the face_model2.nvf file that is now included in the Config/Models folder inside your TouchDesigner installation. Note: Mesh files generated for previous versions of TouchDesigner will no longer work.\nSee also: Face Track SOP\nfacetrackCHOP_Class\n\nContents\n \n \n \n \n \n \n\n\n\n\n\n  active - Enables the face tracking features.\n\n\n\n  modelfolder - The location of the AI model files used for face detection. By default these files are located in the Config/Models folder.\n\n\n\n  meshfile - The 3D morphable mesh file in Nvidia 'nvf' format to use in mesh fitting. When available, the fitted mesh can be accessed with a Face Track SOP.\n\n\n\n  top - A path to the  operator that will provides the image to perform face tracking on.\n\n\n\n  bbox - Output channels that describe a bounding box around the detected face. The channels give the u and v positions of the center of the face as well as the width and height of the box. The positions are relative to the bottom-left corner of the input image.\n\n\n\n  bboxconfidence - Outputs a channel that describes the level of certainty that the AI model has detected a face in the input image. Higher numbers indicate greater confidence.\n\n\n\n  rotations - Output rx, ry, and rz values that indicate how the face is oriented in the image. (0,0,0) indicates that the face is oriented directly towards the camera. Values can range from +/- 180 degrees as the subject turns away from the camera.\n\n\n\n  landmarks -  - \n\n none - The number of facial landmark points to output. Points are numbered beginning from 1 and always represent a fixed feature on the face such as the chin, eyebrow, nose, etc. Positions are given as u and v coordinates relative to the bottom-left corner of the input image. num68 - A standard set of facial landmark features used in AI research. See Reference Diagram. num126 - An extended set of landmark features.\n\n  landmarkconfidence - Adds a confidence value for each landmark feature. Higher values indicate the feature is more likely to be accurate.\n\n\n\n  meshtransform - Enable to output translate, rotate and scale channels for the fitted face mesh. This feature requires a valid 3D morphable face mesh file (see notes above). The values from these channels can be used to transform the mesh produced by an attached Face Track SOP so that it aligns with the input image. By default the fitted mesh is pre-transformed to align with the image, but if 'Pre-Transform' is disabled in the , these values can be used instead for more control and speed.\n\n\n\n  aspectcorrectuv - Rescales the the u and v positions so that they have the correct aspect ratio of the input image. This is useful when using the u, v positions as 3D coordinates rather than as image positions.\n\n\n\n\n\n  timeslice - Turning this on forces the channels to be \"Time Sliced\".  A  is the time between the last cook frame and the current cook frame.\n\n\n\n  scope - To determine which channels get affected, some CHOPs use a  string on the Common page.\n\n\n\n  srselect -  - Handle cases where multiple input CHOPs' sample rates are different. When Resampling occurs, the curves are interpolated according to the Interpolation Method Option, or \"Linear\" if the Interpolate Options are not available.\n\n first - Use rate of first input to resample others. max - Resample to the highest sample rate. min - Resample to the lowest sample rate. err - Doesn't accept conflicting sample rates.\n\n  exportmethod -  - This will determine how to connect the  channel to the parameter. Refer to the Export article for more information.\n\n datindex - Uses the docked  table and references the channel via the index of the channel in the . datname - Uses the docked  table and references the channel via the name of the channel in the . autoname - The channel is the full destination of where to export to, such has geo1/transform1:tx.\n\n  autoexportroot - This path points to the root node where all of the paths that exporting by Channel Name is : are relative to.\n\n\n\n  exporttable - The  used to hold the export information when using the  Table  Methods (See above).\n\n\n\n\n\n\nExtra Information for the Face Track  can be accessed via an Info CHOP.\n\n\n\n - Start of the  interval in samples. - Number of samples in the . - The samplerate of the channels in frames per second. - Number of channels in the . - 1 if  is  enabled, 0 otherwise. - A count of how often the export connections have been updated.\n - Number of times the operator has cooked since the process started. - Duration of the last cook in milliseconds. - Frame number when this operator was last cooked relative to the component timeline. - Frame number when this operator was last cooked relative to the absolute time. - Time in milliseconds at which the operator started cooking in the frame it was cooked. - Time in milliseconds at which the operator finished cooking in the frame it was cooked. - 1 if operator was cooked this frame. - Number of warnings in this operator if any. - Number of errors in this operator if any.\nTouchDesigner Build: Latest\\nwikieditorwikieditorwikieditorwikieditorwikieditorwikieditor2022.24140before 2022.24140\nCHOPs\n• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • \n\nAn Operator Family which operate on Channels (a sequence of numbers (Samples)) which are used for animation, audio, mathematics, simulation, logic, UI construction, and data streamed from/to devices and protocols.\n\n\n\nAn Operator Family that creates, composites and modifies images, and reads/writes images and movies to/from files and the network. TOPs run on the graphics card's GPU.\n\n\n\nA Operator Family that reads, creates and modifies 3D points, polygons, lines, particles, surfaces, spheres and meatballs. Particles and point clouds are now done primarily on the GPU using TOPs.\n\n\n\nA Folder in TouchDesigner always refers to a Windows or macOS operating system directory/folder system that contain files and other folders. It does not refer to operators within TouchDesigner. See Network Path.\n\n\n\nA Time Slice is the time from the last cook frame to the current cook frame. In CHOPs it is the set of short channels that contain the CHOP channels' samples between the last and the current cook frame.\n\n\n\nA parameter in most CHOPs that restricts which channels of that CHOP will be affected. Normally all channels of a CHOP are affected by the operator. TOPs have Channel Mask, a similar feature.\n\n\n\nsamples-per-second of a CHOP. Each CHOP in your network has a sample rate. In contrast, the overall timeline has a Frame Rate, which is the number of frames to cook and display per second, generally your monitor display frequency, default 60.\n\n\n\nExporting is the connection of CHOP channels to parameters of operators. The output of each exporting CHOP is one or more channels, active only while the CHOP Viewer is on. The current value of a channel can be exported to a parameter of any operator, overriding that parameter's value. See Parameter.\n\n\n\nAn Operator Family that manipulates text strings: multi-line text or tables. Multi-line text is often a python Script or GLSL Shader, but can be any multi-line text. Tables are rows and columns of cells, each containing a text string.\n\n\n\nThe location of an operator within the TouchDesigner environment, for example, /geo1/circle1, a node called circle1 in a component called geo1. The path / is called Root. This path is displayed at the top of every Pane, showing which Component's network you are currently in. To refer instead to a filesystem folder, directory, disk file or http: address, see Folder.\n\n\n\nEvery operator in TouchDesigner has a set of control Parameters that can be integer or floating point numbers, menus, binary toggles, text strings or operator paths, which determine the output of the operator.\n\n\n\nTouchDesigner is a hierarchy of components. \"root\" is the top-most network in the hierarchy. The Network Path or Path for root is simply /. A typical path is /project1/moviein1.\n\n\n\n\n\n\n\n\nRetrieved from \"https://docs.derivative.ca/index.php?title=Face_Track_CHOP&oldid=30818\"\n\t\tCategory: CHOPs",
      "tooltip": "",
      "help": "",
      "units": "",
      "examples": [],
      "isReadOnly": false,
      "isAdvanced": false,
      "isHidden": false,
      "isAnimatable": true,
      "isExpression": false,
      "isPython": false,
      "dependsOn": [],
      "affects": [],
      "linkedTo": [],
      "expressionLanguage": "",
      "defaultExpression": "",
      "commonExpressions": [],
      "order": 0,
      "isVisible": true,
      "conditionalDisplay": null,
      "isValid": true,
      "validationErrors": [],
      "lastUpdated": "2025-08-07T07:50:00.587Z",
      "rawData": {},
      "sourceElement": null
    },
    {
      "id": null,
      "name": "Active",
      "label": "Active",
      "group": "General",
      "page": "",
      "type": "float",
      "dataType": "number",
      "style": "",
      "defaultValue": null,
      "minValue": null,
      "maxValue": null,
      "step": null,
      "menuItems": [],
      "menuLabels": [],
      "allowCustom": false,
      "maxLength": null,
      "pattern": null,
      "isArray": false,
      "arraySize": 1,
      "dimensions": 1,
      "description": "From Derivative\n\t\t\n\t\t\n\t\t\n\t\t\n\t\tJump to navigation\n\t\tJump to search\n\t\t\nNOTE\n\nOS: This operator is only supported under the Microsoft Windows operating system.Hardware: This operator uses the Augmented Reality (AR) SDK of the Nvidia Maxine system and requires a 20 series and above Nvidia RTX card to operate.\n\n\n\nThe Face Track  can detect faces and facial landmark points in an image, as well as the direction the face is looking relative to the camera. Using a compatible 3D Morphable Face Model (3DMM) and the Face Track SOP, it can also be used to fit and animate a 3D mesh to the detected face.\nThe input image is taken from a provided  and can be of any resolution or format, and either a still image or video. If multiple faces are present in an image, the  will attempt to track the largest one detected.\nThe coordinates of the detected features are given in u, v positions relative to the bottom-left corner of the input image. By default, the values range from 0 to 1, but the 'Aspect Correct' parameter can be enabled to scale the values so that they can be used as 3D coordinates while maintaining the aspect ratio of the original image.\nTip: Look at the several examples of the Face Track / in OP Snippets.\nTo align a 3D rendering of the points with the original input image, set the 'Projection' of your Camera COMP to 'Orthographic', the 'Ortho Origin' parameter to 'Bottom-Left', and the 'Ortho Width' to 1, while also enabling 'Aspect Correct' on the Face Track .\nTo use the mesh fitting features you will need a compatible face mesh file in the Nvidia 'nvf' format. We recommend using the face_model2.nvf file that is now included in the Config/Models folder inside your TouchDesigner installation. Note: Mesh files generated for previous versions of TouchDesigner will no longer work.\nSee also: Face Track SOP\nfacetrackCHOP_Class\n\nContents\n \n \n \n \n \n \n\n\n\n\n\n  active - Enables the face tracking features.\n\n\n\n  modelfolder - The location of the AI model files used for face detection. By default these files are located in the Config/Models folder.\n\n\n\n  meshfile - The 3D morphable mesh file in Nvidia 'nvf' format to use in mesh fitting. When available, the fitted mesh can be accessed with a Face Track SOP.\n\n\n\n  top - A path to the  operator that will provides the image to perform face tracking on.\n\n\n\n  bbox - Output channels that describe a bounding box around the detected face. The channels give the u and v positions of the center of the face as well as the width and height of the box. The positions are relative to the bottom-left corner of the input image.\n\n\n\n  bboxconfidence - Outputs a channel that describes the level of certainty that the AI model has detected a face in the input image. Higher numbers indicate greater confidence.\n\n\n\n  rotations - Output rx, ry, and rz values that indicate how the face is oriented in the image. (0,0,0) indicates that the face is oriented directly towards the camera. Values can range from +/- 180 degrees as the subject turns away from the camera.\n\n\n\n  landmarks -  - \n\n none - The number of facial landmark points to output. Points are numbered beginning from 1 and always represent a fixed feature on the face such as the chin, eyebrow, nose, etc. Positions are given as u and v coordinates relative to the bottom-left corner of the input image. num68 - A standard set of facial landmark features used in AI research. See Reference Diagram. num126 - An extended set of landmark features.\n\n  landmarkconfidence - Adds a confidence value for each landmark feature. Higher values indicate the feature is more likely to be accurate.\n\n\n\n  meshtransform - Enable to output translate, rotate and scale channels for the fitted face mesh. This feature requires a valid 3D morphable face mesh file (see notes above). The values from these channels can be used to transform the mesh produced by an attached Face Track SOP so that it aligns with the input image. By default the fitted mesh is pre-transformed to align with the image, but if 'Pre-Transform' is disabled in the , these values can be used instead for more control and speed.\n\n\n\n  aspectcorrectuv - Rescales the the u and v positions so that they have the correct aspect ratio of the input image. This is useful when using the u, v positions as 3D coordinates rather than as image positions.\n\n\n\n\n\n  timeslice - Turning this on forces the channels to be \"Time Sliced\".  A  is the time between the last cook frame and the current cook frame.\n\n\n\n  scope - To determine which channels get affected, some CHOPs use a  string on the Common page.\n\n\n\n  srselect -  - Handle cases where multiple input CHOPs' sample rates are different. When Resampling occurs, the curves are interpolated according to the Interpolation Method Option, or \"Linear\" if the Interpolate Options are not available.\n\n first - Use rate of first input to resample others. max - Resample to the highest sample rate. min - Resample to the lowest sample rate. err - Doesn't accept conflicting sample rates.\n\n  exportmethod -  - This will determine how to connect the  channel to the parameter. Refer to the Export article for more information.\n\n datindex - Uses the docked  table and references the channel via the index of the channel in the . datname - Uses the docked  table and references the channel via the name of the channel in the . autoname - The channel is the full destination of where to export to, such has geo1/transform1:tx.\n\n  autoexportroot - This path points to the root node where all of the paths that exporting by Channel Name is : are relative to.\n\n\n\n  exporttable - The  used to hold the export information when using the  Table  Methods (See above).\n\n\n\n\n\n\nExtra Information for the Face Track  can be accessed via an Info CHOP.\n\n\n\n - Start of the  interval in samples. - Number of samples in the . - The samplerate of the channels in frames per second. - Number of channels in the . - 1 if  is  enabled, 0 otherwise. - A count of how often the export connections have been updated.\n - Number of times the operator has cooked since the process started. - Duration of the last cook in milliseconds. - Frame number when this operator was last cooked relative to the component timeline. - Frame number when this operator was last cooked relative to the absolute time. - Time in milliseconds at which the operator started cooking in the frame it was cooked. - Time in milliseconds at which the operator finished cooking in the frame it was cooked. - 1 if operator was cooked this frame. - Number of warnings in this operator if any. - Number of errors in this operator if any.\nTouchDesigner Build: Latest\\nwikieditorwikieditorwikieditorwikieditorwikieditorwikieditor2022.24140before 2022.24140\nCHOPs\n• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • \n\nAn Operator Family which operate on Channels (a sequence of numbers (Samples)) which are used for animation, audio, mathematics, simulation, logic, UI construction, and data streamed from/to devices and protocols.\n\n\n\nAn Operator Family that creates, composites and modifies images, and reads/writes images and movies to/from files and the network. TOPs run on the graphics card's GPU.\n\n\n\nA Operator Family that reads, creates and modifies 3D points, polygons, lines, particles, surfaces, spheres and meatballs. Particles and point clouds are now done primarily on the GPU using TOPs.\n\n\n\nA Folder in TouchDesigner always refers to a Windows or macOS operating system directory/folder system that contain files and other folders. It does not refer to operators within TouchDesigner. See Network Path.\n\n\n\nA Time Slice is the time from the last cook frame to the current cook frame. In CHOPs it is the set of short channels that contain the CHOP channels' samples between the last and the current cook frame.\n\n\n\nA parameter in most CHOPs that restricts which channels of that CHOP will be affected. Normally all channels of a CHOP are affected by the operator. TOPs have Channel Mask, a similar feature.\n\n\n\nsamples-per-second of a CHOP. Each CHOP in your network has a sample rate. In contrast, the overall timeline has a Frame Rate, which is the number of frames to cook and display per second, generally your monitor display frequency, default 60.\n\n\n\nExporting is the connection of CHOP channels to parameters of operators. The output of each exporting CHOP is one or more channels, active only while the CHOP Viewer is on. The current value of a channel can be exported to a parameter of any operator, overriding that parameter's value. See Parameter.\n\n\n\nAn Operator Family that manipulates text strings: multi-line text or tables. Multi-line text is often a python Script or GLSL Shader, but can be any multi-line text. Tables are rows and columns of cells, each containing a text string.\n\n\n\nThe location of an operator within the TouchDesigner environment, for example, /geo1/circle1, a node called circle1 in a component called geo1. The path / is called Root. This path is displayed at the top of every Pane, showing which Component's network you are currently in. To refer instead to a filesystem folder, directory, disk file or http: address, see Folder.\n\n\n\nEvery operator in TouchDesigner has a set of control Parameters that can be integer or floating point numbers, menus, binary toggles, text strings or operator paths, which determine the output of the operator.\n\n\n\nTouchDesigner is a hierarchy of components. \"root\" is the top-most network in the hierarchy. The Network Path or Path for root is simply /. A typical path is /project1/moviein1.\n\n\n\n\n\n\n\n\nRetrieved from \"https://docs.derivative.ca/index.php?title=Face_Track_CHOP&oldid=30818\"\n\t\tCategory: CHOPs",
      "tooltip": "",
      "help": "",
      "units": "",
      "examples": [],
      "isReadOnly": false,
      "isAdvanced": false,
      "isHidden": false,
      "isAnimatable": true,
      "isExpression": false,
      "isPython": false,
      "dependsOn": [],
      "affects": [],
      "linkedTo": [],
      "expressionLanguage": "",
      "defaultExpression": "",
      "commonExpressions": [],
      "order": 0,
      "isVisible": true,
      "conditionalDisplay": null,
      "isValid": true,
      "validationErrors": [],
      "lastUpdated": "2025-08-07T07:50:00.590Z",
      "rawData": {},
      "sourceElement": null
    },
    {
      "id": null,
      "name": "Active",
      "label": "Active",
      "group": "General",
      "page": "",
      "type": "float",
      "dataType": "number",
      "style": "",
      "defaultValue": null,
      "minValue": null,
      "maxValue": null,
      "step": null,
      "menuItems": [],
      "menuLabels": [],
      "allowCustom": false,
      "maxLength": null,
      "pattern": null,
      "isArray": false,
      "arraySize": 1,
      "dimensions": 1,
      "description": "NOTE\n\nOS: This operator is only supported under the Microsoft Windows operating system.Hardware: This operator uses the Augmented Reality (AR) SDK of the Nvidia Maxine system and requires a 20 series and above Nvidia RTX card to operate.\n\n\n\nThe Face Track  can detect faces and facial landmark points in an image, as well as the direction the face is looking relative to the camera. Using a compatible 3D Morphable Face Model (3DMM) and the Face Track SOP, it can also be used to fit and animate a 3D mesh to the detected face.\nThe input image is taken from a provided  and can be of any resolution or format, and either a still image or video. If multiple faces are present in an image, the  will attempt to track the largest one detected.\nThe coordinates of the detected features are given in u, v positions relative to the bottom-left corner of the input image. By default, the values range from 0 to 1, but the 'Aspect Correct' parameter can be enabled to scale the values so that they can be used as 3D coordinates while maintaining the aspect ratio of the original image.\nTip: Look at the several examples of the Face Track / in OP Snippets.\nTo align a 3D rendering of the points with the original input image, set the 'Projection' of your Camera COMP to 'Orthographic', the 'Ortho Origin' parameter to 'Bottom-Left', and the 'Ortho Width' to 1, while also enabling 'Aspect Correct' on the Face Track .\nTo use the mesh fitting features you will need a compatible face mesh file in the Nvidia 'nvf' format. We recommend using the face_model2.nvf file that is now included in the Config/Models folder inside your TouchDesigner installation. Note: Mesh files generated for previous versions of TouchDesigner will no longer work.\nSee also: Face Track SOP\nfacetrackCHOP_Class\n\nContents\n \n \n \n \n \n \n\n\n\n\n\n  active - Enables the face tracking features.\n\n\n\n  modelfolder - The location of the AI model files used for face detection. By default these files are located in the Config/Models folder.\n\n\n\n  meshfile - The 3D morphable mesh file in Nvidia 'nvf' format to use in mesh fitting. When available, the fitted mesh can be accessed with a Face Track SOP.\n\n\n\n  top - A path to the  operator that will provides the image to perform face tracking on.\n\n\n\n  bbox - Output channels that describe a bounding box around the detected face. The channels give the u and v positions of the center of the face as well as the width and height of the box. The positions are relative to the bottom-left corner of the input image.\n\n\n\n  bboxconfidence - Outputs a channel that describes the level of certainty that the AI model has detected a face in the input image. Higher numbers indicate greater confidence.\n\n\n\n  rotations - Output rx, ry, and rz values that indicate how the face is oriented in the image. (0,0,0) indicates that the face is oriented directly towards the camera. Values can range from +/- 180 degrees as the subject turns away from the camera.\n\n\n\n  landmarks -  - \n\n none - The number of facial landmark points to output. Points are numbered beginning from 1 and always represent a fixed feature on the face such as the chin, eyebrow, nose, etc. Positions are given as u and v coordinates relative to the bottom-left corner of the input image. num68 - A standard set of facial landmark features used in AI research. See Reference Diagram. num126 - An extended set of landmark features.\n\n  landmarkconfidence - Adds a confidence value for each landmark feature. Higher values indicate the feature is more likely to be accurate.\n\n\n\n  meshtransform - Enable to output translate, rotate and scale channels for the fitted face mesh. This feature requires a valid 3D morphable face mesh file (see notes above). The values from these channels can be used to transform the mesh produced by an attached Face Track SOP so that it aligns with the input image. By default the fitted mesh is pre-transformed to align with the image, but if 'Pre-Transform' is disabled in the , these values can be used instead for more control and speed.\n\n\n\n  aspectcorrectuv - Rescales the the u and v positions so that they have the correct aspect ratio of the input image. This is useful when using the u, v positions as 3D coordinates rather than as image positions.\n\n\n\n\n\n  timeslice - Turning this on forces the channels to be \"Time Sliced\".  A  is the time between the last cook frame and the current cook frame.\n\n\n\n  scope - To determine which channels get affected, some CHOPs use a  string on the Common page.\n\n\n\n  srselect -  - Handle cases where multiple input CHOPs' sample rates are different. When Resampling occurs, the curves are interpolated according to the Interpolation Method Option, or \"Linear\" if the Interpolate Options are not available.\n\n first - Use rate of first input to resample others. max - Resample to the highest sample rate. min - Resample to the lowest sample rate. err - Doesn't accept conflicting sample rates.\n\n  exportmethod -  - This will determine how to connect the  channel to the parameter. Refer to the Export article for more information.\n\n datindex - Uses the docked  table and references the channel via the index of the channel in the . datname - Uses the docked  table and references the channel via the name of the channel in the . autoname - The channel is the full destination of where to export to, such has geo1/transform1:tx.\n\n  autoexportroot - This path points to the root node where all of the paths that exporting by Channel Name is : are relative to.\n\n\n\n  exporttable - The  used to hold the export information when using the  Table  Methods (See above).\n\n\n\n\n\n\nExtra Information for the Face Track  can be accessed via an Info CHOP.\n\n\n\n - Start of the  interval in samples. - Number of samples in the . - The samplerate of the channels in frames per second. - Number of channels in the . - 1 if  is  enabled, 0 otherwise. - A count of how often the export connections have been updated.\n - Number of times the operator has cooked since the process started. - Duration of the last cook in milliseconds. - Frame number when this operator was last cooked relative to the component timeline. - Frame number when this operator was last cooked relative to the absolute time. - Time in milliseconds at which the operator started cooking in the frame it was cooked. - Time in milliseconds at which the operator finished cooking in the frame it was cooked. - 1 if operator was cooked this frame. - Number of warnings in this operator if any. - Number of errors in this operator if any.\nTouchDesigner Build: Latest\\nwikieditorwikieditorwikieditorwikieditorwikieditorwikieditor2022.24140before 2022.24140\nCHOPs\n• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • \n\nAn Operator Family which operate on Channels (a sequence of numbers (Samples)) which are used for animation, audio, mathematics, simulation, logic, UI construction, and data streamed from/to devices and protocols.\n\n\n\nAn Operator Family that creates, composites and modifies images, and reads/writes images and movies to/from files and the network. TOPs run on the graphics card's GPU.\n\n\n\nA Operator Family that reads, creates and modifies 3D points, polygons, lines, particles, surfaces, spheres and meatballs. Particles and point clouds are now done primarily on the GPU using TOPs.\n\n\n\nA Folder in TouchDesigner always refers to a Windows or macOS operating system directory/folder system that contain files and other folders. It does not refer to operators within TouchDesigner. See Network Path.\n\n\n\nA Time Slice is the time from the last cook frame to the current cook frame. In CHOPs it is the set of short channels that contain the CHOP channels' samples between the last and the current cook frame.\n\n\n\nA parameter in most CHOPs that restricts which channels of that CHOP will be affected. Normally all channels of a CHOP are affected by the operator. TOPs have Channel Mask, a similar feature.\n\n\n\nsamples-per-second of a CHOP. Each CHOP in your network has a sample rate. In contrast, the overall timeline has a Frame Rate, which is the number of frames to cook and display per second, generally your monitor display frequency, default 60.\n\n\n\nExporting is the connection of CHOP channels to parameters of operators. The output of each exporting CHOP is one or more channels, active only while the CHOP Viewer is on. The current value of a channel can be exported to a parameter of any operator, overriding that parameter's value. See Parameter.\n\n\n\nAn Operator Family that manipulates text strings: multi-line text or tables. Multi-line text is often a python Script or GLSL Shader, but can be any multi-line text. Tables are rows and columns of cells, each containing a text string.\n\n\n\nThe location of an operator within the TouchDesigner environment, for example, /geo1/circle1, a node called circle1 in a component called geo1. The path / is called Root. This path is displayed at the top of every Pane, showing which Component's network you are currently in. To refer instead to a filesystem folder, directory, disk file or http: address, see Folder.\n\n\n\nEvery operator in TouchDesigner has a set of control Parameters that can be integer or floating point numbers, menus, binary toggles, text strings or operator paths, which determine the output of the operator.\n\n\n\nTouchDesigner is a hierarchy of components. \"root\" is the top-most network in the hierarchy. The Network Path or Path for root is simply /. A typical path is /project1/moviein1.\n\n\n\n\n\n\n\n\nRetrieved from \"https://docs.derivative.ca/index.php?title=Face_Track_CHOP&oldid=30818\"",
      "tooltip": "",
      "help": "",
      "units": "",
      "examples": [],
      "isReadOnly": false,
      "isAdvanced": false,
      "isHidden": false,
      "isAnimatable": true,
      "isExpression": false,
      "isPython": false,
      "dependsOn": [],
      "affects": [],
      "linkedTo": [],
      "expressionLanguage": "",
      "defaultExpression": "",
      "commonExpressions": [],
      "order": 0,
      "isVisible": true,
      "conditionalDisplay": null,
      "isValid": true,
      "validationErrors": [],
      "lastUpdated": "2025-08-07T07:50:00.595Z",
      "rawData": {},
      "sourceElement": null
    },
    {
      "id": null,
      "name": "Active",
      "label": "Active",
      "group": "General",
      "page": "",
      "type": "float",
      "dataType": "number",
      "style": "",
      "defaultValue": null,
      "minValue": null,
      "maxValue": null,
      "step": null,
      "menuItems": [],
      "menuLabels": [],
      "allowCustom": false,
      "maxLength": null,
      "pattern": null,
      "isArray": false,
      "arraySize": 1,
      "dimensions": 1,
      "description": "active - Enables the face tracking features.",
      "tooltip": "",
      "help": "",
      "units": "",
      "examples": [],
      "isReadOnly": false,
      "isAdvanced": false,
      "isHidden": false,
      "isAnimatable": true,
      "isExpression": false,
      "isPython": false,
      "dependsOn": [],
      "affects": [],
      "linkedTo": [],
      "expressionLanguage": "",
      "defaultExpression": "",
      "commonExpressions": [],
      "order": 0,
      "isVisible": true,
      "conditionalDisplay": null,
      "isValid": true,
      "validationErrors": [],
      "lastUpdated": "2025-08-07T07:50:00.595Z",
      "rawData": {},
      "sourceElement": null
    },
    {
      "id": null,
      "name": "Model Folder",
      "label": "Model Folder",
      "group": "General",
      "page": "",
      "type": "float",
      "dataType": "number",
      "style": "",
      "defaultValue": null,
      "minValue": null,
      "maxValue": null,
      "step": null,
      "menuItems": [],
      "menuLabels": [],
      "allowCustom": false,
      "maxLength": null,
      "pattern": null,
      "isArray": false,
      "arraySize": 1,
      "dimensions": 1,
      "description": "modelfolder - The location of the AI model files used for face detection. By default these files are located in the Config/Models folder.",
      "tooltip": "",
      "help": "",
      "units": "",
      "examples": [],
      "isReadOnly": false,
      "isAdvanced": false,
      "isHidden": false,
      "isAnimatable": true,
      "isExpression": false,
      "isPython": false,
      "dependsOn": [],
      "affects": [],
      "linkedTo": [],
      "expressionLanguage": "",
      "defaultExpression": "",
      "commonExpressions": [],
      "order": 0,
      "isVisible": true,
      "conditionalDisplay": null,
      "isValid": true,
      "validationErrors": [],
      "lastUpdated": "2025-08-07T07:50:00.595Z",
      "rawData": {},
      "sourceElement": null
    },
    {
      "id": null,
      "name": "Mesh File",
      "label": "Mesh File",
      "group": "General",
      "page": "",
      "type": "float",
      "dataType": "number",
      "style": "",
      "defaultValue": null,
      "minValue": null,
      "maxValue": null,
      "step": null,
      "menuItems": [],
      "menuLabels": [],
      "allowCustom": false,
      "maxLength": null,
      "pattern": null,
      "isArray": false,
      "arraySize": 1,
      "dimensions": 1,
      "description": "meshfile - The 3D morphable mesh file in Nvidia 'nvf' format to use in mesh fitting. When available, the fitted mesh can be accessed with a Face Track SOP.",
      "tooltip": "",
      "help": "",
      "units": "",
      "examples": [],
      "isReadOnly": false,
      "isAdvanced": false,
      "isHidden": false,
      "isAnimatable": true,
      "isExpression": false,
      "isPython": false,
      "dependsOn": [],
      "affects": [],
      "linkedTo": [],
      "expressionLanguage": "",
      "defaultExpression": "",
      "commonExpressions": [],
      "order": 0,
      "isVisible": true,
      "conditionalDisplay": null,
      "isValid": true,
      "validationErrors": [],
      "lastUpdated": "2025-08-07T07:50:00.595Z",
      "rawData": {},
      "sourceElement": null
    },
    {
      "id": null,
      "name": "TOP",
      "label": "TOP",
      "group": "General",
      "page": "",
      "type": "float",
      "dataType": "number",
      "style": "",
      "defaultValue": null,
      "minValue": null,
      "maxValue": null,
      "step": null,
      "menuItems": [],
      "menuLabels": [],
      "allowCustom": false,
      "maxLength": null,
      "pattern": null,
      "isArray": false,
      "arraySize": 1,
      "dimensions": 1,
      "description": "top - A path to the  operator that will provides the image to perform face tracking on.",
      "tooltip": "",
      "help": "",
      "units": "",
      "examples": [],
      "isReadOnly": false,
      "isAdvanced": false,
      "isHidden": false,
      "isAnimatable": true,
      "isExpression": false,
      "isPython": false,
      "dependsOn": [],
      "affects": [],
      "linkedTo": [],
      "expressionLanguage": "",
      "defaultExpression": "",
      "commonExpressions": [],
      "order": 0,
      "isVisible": true,
      "conditionalDisplay": null,
      "isValid": true,
      "validationErrors": [],
      "lastUpdated": "2025-08-07T07:50:00.596Z",
      "rawData": {},
      "sourceElement": null
    },
    {
      "id": null,
      "name": "Bounding Boxes",
      "label": "Bounding Boxes",
      "group": "General",
      "page": "",
      "type": "float",
      "dataType": "number",
      "style": "",
      "defaultValue": null,
      "minValue": null,
      "maxValue": null,
      "step": null,
      "menuItems": [],
      "menuLabels": [],
      "allowCustom": false,
      "maxLength": null,
      "pattern": null,
      "isArray": false,
      "arraySize": 1,
      "dimensions": 1,
      "description": "bbox - Output channels that describe a bounding box around the detected face. The channels give the u and v positions of the center of the face as well as the width and height of the box. The positions are relative to the bottom-left corner of the input image.",
      "tooltip": "",
      "help": "",
      "units": "",
      "examples": [],
      "isReadOnly": false,
      "isAdvanced": false,
      "isHidden": false,
      "isAnimatable": true,
      "isExpression": false,
      "isPython": false,
      "dependsOn": [],
      "affects": [],
      "linkedTo": [],
      "expressionLanguage": "",
      "defaultExpression": "",
      "commonExpressions": [],
      "order": 0,
      "isVisible": true,
      "conditionalDisplay": null,
      "isValid": true,
      "validationErrors": [],
      "lastUpdated": "2025-08-07T07:50:00.596Z",
      "rawData": {},
      "sourceElement": null
    },
    {
      "id": null,
      "name": "Bounding Box Confidence",
      "label": "Bounding Box Confidence",
      "group": "General",
      "page": "",
      "type": "float",
      "dataType": "number",
      "style": "",
      "defaultValue": null,
      "minValue": null,
      "maxValue": null,
      "step": null,
      "menuItems": [],
      "menuLabels": [],
      "allowCustom": false,
      "maxLength": null,
      "pattern": null,
      "isArray": false,
      "arraySize": 1,
      "dimensions": 1,
      "description": "bboxconfidence - Outputs a channel that describes the level of certainty that the AI model has detected a face in the input image. Higher numbers indicate greater confidence.",
      "tooltip": "",
      "help": "",
      "units": "",
      "examples": [],
      "isReadOnly": false,
      "isAdvanced": false,
      "isHidden": false,
      "isAnimatable": true,
      "isExpression": false,
      "isPython": false,
      "dependsOn": [],
      "affects": [],
      "linkedTo": [],
      "expressionLanguage": "",
      "defaultExpression": "",
      "commonExpressions": [],
      "order": 0,
      "isVisible": true,
      "conditionalDisplay": null,
      "isValid": true,
      "validationErrors": [],
      "lastUpdated": "2025-08-07T07:50:00.596Z",
      "rawData": {},
      "sourceElement": null
    },
    {
      "id": null,
      "name": "Rotations",
      "label": "Rotations",
      "group": "General",
      "page": "",
      "type": "float",
      "dataType": "number",
      "style": "",
      "defaultValue": null,
      "minValue": null,
      "maxValue": null,
      "step": null,
      "menuItems": [],
      "menuLabels": [],
      "allowCustom": false,
      "maxLength": null,
      "pattern": null,
      "isArray": false,
      "arraySize": 1,
      "dimensions": 1,
      "description": "rotations - Output rx, ry, and rz values that indicate how the face is oriented in the image. (0,0,0) indicates that the face is oriented directly towards the camera. Values can range from +/- 180 degrees as the subject turns away from the camera.",
      "tooltip": "",
      "help": "",
      "units": "",
      "examples": [],
      "isReadOnly": false,
      "isAdvanced": false,
      "isHidden": false,
      "isAnimatable": true,
      "isExpression": false,
      "isPython": false,
      "dependsOn": [],
      "affects": [],
      "linkedTo": [],
      "expressionLanguage": "",
      "defaultExpression": "",
      "commonExpressions": [],
      "order": 0,
      "isVisible": true,
      "conditionalDisplay": null,
      "isValid": true,
      "validationErrors": [],
      "lastUpdated": "2025-08-07T07:50:00.596Z",
      "rawData": {},
      "sourceElement": null
    },
    {
      "id": null,
      "name": "Number of Landmarks",
      "label": "Number of Landmarks",
      "group": "General",
      "page": "",
      "type": "float",
      "dataType": "number",
      "style": "",
      "defaultValue": null,
      "minValue": null,
      "maxValue": null,
      "step": null,
      "menuItems": [],
      "menuLabels": [],
      "allowCustom": false,
      "maxLength": null,
      "pattern": null,
      "isArray": false,
      "arraySize": 1,
      "dimensions": 1,
      "description": "landmarks -  - \n\n none - The number of facial landmark points to output. Points are numbered beginning from 1 and always represent a fixed feature on the face such as the chin, eyebrow, nose, etc. Positions are given as u and v coordinates relative to the bottom-left corner of the input image. num68 - A standard set of facial landmark features used in AI research. See Reference Diagram. num126 - An extended set of landmark features.",
      "tooltip": "",
      "help": "",
      "units": "",
      "examples": [],
      "isReadOnly": false,
      "isAdvanced": false,
      "isHidden": false,
      "isAnimatable": true,
      "isExpression": false,
      "isPython": false,
      "dependsOn": [],
      "affects": [],
      "linkedTo": [],
      "expressionLanguage": "",
      "defaultExpression": "",
      "commonExpressions": [],
      "order": 0,
      "isVisible": true,
      "conditionalDisplay": null,
      "isValid": true,
      "validationErrors": [],
      "lastUpdated": "2025-08-07T07:50:00.596Z",
      "rawData": {},
      "sourceElement": null
    },
    {
      "id": null,
      "name": "None",
      "label": "None",
      "group": "General",
      "page": "",
      "type": "float",
      "dataType": "number",
      "style": "",
      "defaultValue": null,
      "minValue": null,
      "maxValue": null,
      "step": null,
      "menuItems": [],
      "menuLabels": [],
      "allowCustom": false,
      "maxLength": null,
      "pattern": null,
      "isArray": false,
      "arraySize": 1,
      "dimensions": 1,
      "description": "none - The number of facial landmark points to output. Points are numbered beginning from 1 and always represent a fixed feature on the face such as the chin, eyebrow, nose, etc. Positions are given as u and v coordinates relative to the bottom-left corner of the input image. num68 - A standard set of facial landmark features used in AI research. See Reference Diagram. num126 - An extended set of landmark features.",
      "tooltip": "",
      "help": "",
      "units": "",
      "examples": [],
      "isReadOnly": false,
      "isAdvanced": false,
      "isHidden": false,
      "isAnimatable": true,
      "isExpression": false,
      "isPython": false,
      "dependsOn": [],
      "affects": [],
      "linkedTo": [],
      "expressionLanguage": "",
      "defaultExpression": "",
      "commonExpressions": [],
      "order": 0,
      "isVisible": true,
      "conditionalDisplay": null,
      "isValid": true,
      "validationErrors": [],
      "lastUpdated": "2025-08-07T07:50:00.596Z",
      "rawData": {},
      "sourceElement": null
    },
    {
      "id": null,
      "name": "Landmark Confidence",
      "label": "Landmark Confidence",
      "group": "General",
      "page": "",
      "type": "float",
      "dataType": "number",
      "style": "",
      "defaultValue": null,
      "minValue": null,
      "maxValue": null,
      "step": null,
      "menuItems": [],
      "menuLabels": [],
      "allowCustom": false,
      "maxLength": null,
      "pattern": null,
      "isArray": false,
      "arraySize": 1,
      "dimensions": 1,
      "description": "landmarkconfidence - Adds a confidence value for each landmark feature. Higher values indicate the feature is more likely to be accurate.",
      "tooltip": "",
      "help": "",
      "units": "",
      "examples": [],
      "isReadOnly": false,
      "isAdvanced": false,
      "isHidden": false,
      "isAnimatable": true,
      "isExpression": false,
      "isPython": false,
      "dependsOn": [],
      "affects": [],
      "linkedTo": [],
      "expressionLanguage": "",
      "defaultExpression": "",
      "commonExpressions": [],
      "order": 0,
      "isVisible": true,
      "conditionalDisplay": null,
      "isValid": true,
      "validationErrors": [],
      "lastUpdated": "2025-08-07T07:50:00.596Z",
      "rawData": {},
      "sourceElement": null
    },
    {
      "id": null,
      "name": "Mesh Transform",
      "label": "Mesh Transform",
      "group": "General",
      "page": "",
      "type": "float",
      "dataType": "number",
      "style": "",
      "defaultValue": null,
      "minValue": null,
      "maxValue": null,
      "step": null,
      "menuItems": [],
      "menuLabels": [],
      "allowCustom": false,
      "maxLength": null,
      "pattern": null,
      "isArray": false,
      "arraySize": 1,
      "dimensions": 1,
      "description": "meshtransform - Enable to output translate, rotate and scale channels for the fitted face mesh. This feature requires a valid 3D morphable face mesh file (see notes above). The values from these channels can be used to transform the mesh produced by an attached Face Track SOP so that it aligns with the input image. By default the fitted mesh is pre-transformed to align with the image, but if 'Pre-Transform' is disabled in the , these values can be used instead for more control and speed.",
      "tooltip": "",
      "help": "",
      "units": "",
      "examples": [],
      "isReadOnly": false,
      "isAdvanced": false,
      "isHidden": false,
      "isAnimatable": true,
      "isExpression": false,
      "isPython": false,
      "dependsOn": [],
      "affects": [],
      "linkedTo": [],
      "expressionLanguage": "",
      "defaultExpression": "",
      "commonExpressions": [],
      "order": 0,
      "isVisible": true,
      "conditionalDisplay": null,
      "isValid": true,
      "validationErrors": [],
      "lastUpdated": "2025-08-07T07:50:00.596Z",
      "rawData": {},
      "sourceElement": null
    },
    {
      "id": null,
      "name": "Aspect Correct UVs",
      "label": "Aspect Correct UVs",
      "group": "General",
      "page": "",
      "type": "float",
      "dataType": "number",
      "style": "",
      "defaultValue": null,
      "minValue": null,
      "maxValue": null,
      "step": null,
      "menuItems": [],
      "menuLabels": [],
      "allowCustom": false,
      "maxLength": null,
      "pattern": null,
      "isArray": false,
      "arraySize": 1,
      "dimensions": 1,
      "description": "aspectcorrectuv - Rescales the the u and v positions so that they have the correct aspect ratio of the input image. This is useful when using the u, v positions as 3D coordinates rather than as image positions.",
      "tooltip": "",
      "help": "",
      "units": "",
      "examples": [],
      "isReadOnly": false,
      "isAdvanced": false,
      "isHidden": false,
      "isAnimatable": true,
      "isExpression": false,
      "isPython": false,
      "dependsOn": [],
      "affects": [],
      "linkedTo": [],
      "expressionLanguage": "",
      "defaultExpression": "",
      "commonExpressions": [],
      "order": 0,
      "isVisible": true,
      "conditionalDisplay": null,
      "isValid": true,
      "validationErrors": [],
      "lastUpdated": "2025-08-07T07:50:00.596Z",
      "rawData": {},
      "sourceElement": null
    },
    {
      "id": null,
      "name": "Time Slice",
      "label": "Time Slice",
      "group": "General",
      "page": "",
      "type": "float",
      "dataType": "number",
      "style": "",
      "defaultValue": null,
      "minValue": null,
      "maxValue": null,
      "step": null,
      "menuItems": [],
      "menuLabels": [],
      "allowCustom": false,
      "maxLength": null,
      "pattern": null,
      "isArray": false,
      "arraySize": 1,
      "dimensions": 1,
      "description": "timeslice - Turning this on forces the channels to be \"Time Sliced\".  A  is the time between the last cook frame and the current cook frame.",
      "tooltip": "",
      "help": "",
      "units": "",
      "examples": [],
      "isReadOnly": false,
      "isAdvanced": false,
      "isHidden": false,
      "isAnimatable": true,
      "isExpression": false,
      "isPython": false,
      "dependsOn": [],
      "affects": [],
      "linkedTo": [],
      "expressionLanguage": "",
      "defaultExpression": "",
      "commonExpressions": [],
      "order": 0,
      "isVisible": true,
      "conditionalDisplay": null,
      "isValid": true,
      "validationErrors": [],
      "lastUpdated": "2025-08-07T07:50:00.596Z",
      "rawData": {},
      "sourceElement": null
    },
    {
      "id": null,
      "name": "Scope",
      "label": "Scope",
      "group": "General",
      "page": "",
      "type": "float",
      "dataType": "number",
      "style": "",
      "defaultValue": null,
      "minValue": null,
      "maxValue": null,
      "step": null,
      "menuItems": [],
      "menuLabels": [],
      "allowCustom": false,
      "maxLength": null,
      "pattern": null,
      "isArray": false,
      "arraySize": 1,
      "dimensions": 1,
      "description": "scope - To determine which channels get affected, some CHOPs use a  string on the Common page.",
      "tooltip": "",
      "help": "",
      "units": "",
      "examples": [],
      "isReadOnly": false,
      "isAdvanced": false,
      "isHidden": false,
      "isAnimatable": true,
      "isExpression": false,
      "isPython": false,
      "dependsOn": [],
      "affects": [],
      "linkedTo": [],
      "expressionLanguage": "",
      "defaultExpression": "",
      "commonExpressions": [],
      "order": 0,
      "isVisible": true,
      "conditionalDisplay": null,
      "isValid": true,
      "validationErrors": [],
      "lastUpdated": "2025-08-07T07:50:00.596Z",
      "rawData": {},
      "sourceElement": null
    },
    {
      "id": null,
      "name": "Sample Rate Match",
      "label": "Sample Rate Match",
      "group": "General",
      "page": "",
      "type": "float",
      "dataType": "number",
      "style": "",
      "defaultValue": null,
      "minValue": null,
      "maxValue": null,
      "step": null,
      "menuItems": [],
      "menuLabels": [],
      "allowCustom": false,
      "maxLength": null,
      "pattern": null,
      "isArray": false,
      "arraySize": 1,
      "dimensions": 1,
      "description": "srselect -  - Handle cases where multiple input CHOPs' sample rates are different. When Resampling occurs, the curves are interpolated according to the Interpolation Method Option, or \"Linear\" if the Interpolate Options are not available.\n\n first - Use rate of first input to resample others. max - Resample to the highest sample rate. min - Resample to the lowest sample rate. err - Doesn't accept conflicting sample rates.",
      "tooltip": "",
      "help": "",
      "units": "",
      "examples": [],
      "isReadOnly": false,
      "isAdvanced": false,
      "isHidden": false,
      "isAnimatable": true,
      "isExpression": false,
      "isPython": false,
      "dependsOn": [],
      "affects": [],
      "linkedTo": [],
      "expressionLanguage": "",
      "defaultExpression": "",
      "commonExpressions": [],
      "order": 0,
      "isVisible": true,
      "conditionalDisplay": null,
      "isValid": true,
      "validationErrors": [],
      "lastUpdated": "2025-08-07T07:50:00.596Z",
      "rawData": {},
      "sourceElement": null
    },
    {
      "id": null,
      "name": "Resample At First Input's Rate",
      "label": "Resample At First Input's Rate",
      "group": "General",
      "page": "",
      "type": "float",
      "dataType": "number",
      "style": "",
      "defaultValue": null,
      "minValue": null,
      "maxValue": null,
      "step": null,
      "menuItems": [],
      "menuLabels": [],
      "allowCustom": false,
      "maxLength": null,
      "pattern": null,
      "isArray": false,
      "arraySize": 1,
      "dimensions": 1,
      "description": "first - Use rate of first input to resample others. max - Resample to the highest sample rate. min - Resample to the lowest sample rate. err - Doesn't accept conflicting sample rates.",
      "tooltip": "",
      "help": "",
      "units": "",
      "examples": [],
      "isReadOnly": false,
      "isAdvanced": false,
      "isHidden": false,
      "isAnimatable": true,
      "isExpression": false,
      "isPython": false,
      "dependsOn": [],
      "affects": [],
      "linkedTo": [],
      "expressionLanguage": "",
      "defaultExpression": "",
      "commonExpressions": [],
      "order": 0,
      "isVisible": true,
      "conditionalDisplay": null,
      "isValid": true,
      "validationErrors": [],
      "lastUpdated": "2025-08-07T07:50:00.596Z",
      "rawData": {},
      "sourceElement": null
    },
    {
      "id": null,
      "name": "Export Method",
      "label": "Export Method",
      "group": "General",
      "page": "",
      "type": "float",
      "dataType": "number",
      "style": "",
      "defaultValue": null,
      "minValue": null,
      "maxValue": null,
      "step": null,
      "menuItems": [],
      "menuLabels": [],
      "allowCustom": false,
      "maxLength": null,
      "pattern": null,
      "isArray": false,
      "arraySize": 1,
      "dimensions": 1,
      "description": "exportmethod -  - This will determine how to connect the  channel to the parameter. Refer to the Export article for more information.\n\n datindex - Uses the docked  table and references the channel via the index of the channel in the . datname - Uses the docked  table and references the channel via the name of the channel in the . autoname - The channel is the full destination of where to export to, such has geo1/transform1:tx.",
      "tooltip": "",
      "help": "",
      "units": "",
      "examples": [],
      "isReadOnly": false,
      "isAdvanced": false,
      "isHidden": false,
      "isAnimatable": true,
      "isExpression": false,
      "isPython": false,
      "dependsOn": [],
      "affects": [],
      "linkedTo": [],
      "expressionLanguage": "",
      "defaultExpression": "",
      "commonExpressions": [],
      "order": 0,
      "isVisible": true,
      "conditionalDisplay": null,
      "isValid": true,
      "validationErrors": [],
      "lastUpdated": "2025-08-07T07:50:00.597Z",
      "rawData": {},
      "sourceElement": null
    },
    {
      "id": null,
      "name": "DAT Table by Index",
      "label": "DAT Table by Index",
      "group": "General",
      "page": "",
      "type": "float",
      "dataType": "number",
      "style": "",
      "defaultValue": null,
      "minValue": null,
      "maxValue": null,
      "step": null,
      "menuItems": [],
      "menuLabels": [],
      "allowCustom": false,
      "maxLength": null,
      "pattern": null,
      "isArray": false,
      "arraySize": 1,
      "dimensions": 1,
      "description": "datindex - Uses the docked  table and references the channel via the index of the channel in the . datname - Uses the docked  table and references the channel via the name of the channel in the . autoname - The channel is the full destination of where to export to, such has geo1/transform1:tx.",
      "tooltip": "",
      "help": "",
      "units": "",
      "examples": [],
      "isReadOnly": false,
      "isAdvanced": false,
      "isHidden": false,
      "isAnimatable": true,
      "isExpression": false,
      "isPython": false,
      "dependsOn": [],
      "affects": [],
      "linkedTo": [],
      "expressionLanguage": "",
      "defaultExpression": "",
      "commonExpressions": [],
      "order": 0,
      "isVisible": true,
      "conditionalDisplay": null,
      "isValid": true,
      "validationErrors": [],
      "lastUpdated": "2025-08-07T07:50:00.597Z",
      "rawData": {},
      "sourceElement": null
    },
    {
      "id": null,
      "name": "Export Root",
      "label": "Export Root",
      "group": "General",
      "page": "",
      "type": "float",
      "dataType": "number",
      "style": "",
      "defaultValue": null,
      "minValue": null,
      "maxValue": null,
      "step": null,
      "menuItems": [],
      "menuLabels": [],
      "allowCustom": false,
      "maxLength": null,
      "pattern": null,
      "isArray": false,
      "arraySize": 1,
      "dimensions": 1,
      "description": "autoexportroot - This path points to the root node where all of the paths that exporting by Channel Name is : are relative to.",
      "tooltip": "",
      "help": "",
      "units": "",
      "examples": [],
      "isReadOnly": false,
      "isAdvanced": false,
      "isHidden": false,
      "isAnimatable": true,
      "isExpression": false,
      "isPython": false,
      "dependsOn": [],
      "affects": [],
      "linkedTo": [],
      "expressionLanguage": "",
      "defaultExpression": "",
      "commonExpressions": [],
      "order": 0,
      "isVisible": true,
      "conditionalDisplay": null,
      "isValid": true,
      "validationErrors": [],
      "lastUpdated": "2025-08-07T07:50:00.597Z",
      "rawData": {},
      "sourceElement": null
    },
    {
      "id": null,
      "name": "Export Table",
      "label": "Export Table",
      "group": "General",
      "page": "",
      "type": "float",
      "dataType": "number",
      "style": "",
      "defaultValue": null,
      "minValue": null,
      "maxValue": null,
      "step": null,
      "menuItems": [],
      "menuLabels": [],
      "allowCustom": false,
      "maxLength": null,
      "pattern": null,
      "isArray": false,
      "arraySize": 1,
      "dimensions": 1,
      "description": "exporttable - The  used to hold the export information when using the  Table  Methods (See above).",
      "tooltip": "",
      "help": "",
      "units": "",
      "examples": [],
      "isReadOnly": false,
      "isAdvanced": false,
      "isHidden": false,
      "isAnimatable": true,
      "isExpression": false,
      "isPython": false,
      "dependsOn": [],
      "affects": [],
      "linkedTo": [],
      "expressionLanguage": "",
      "defaultExpression": "",
      "commonExpressions": [],
      "order": 0,
      "isVisible": true,
      "conditionalDisplay": null,
      "isValid": true,
      "validationErrors": [],
      "lastUpdated": "2025-08-07T07:50:00.597Z",
      "rawData": {},
      "sourceElement": null
    }
  ],
  "parameterGroups": {},
  "codeExamples": [],
  "pythonExamples": [],
  "expressions": [],
  "commonInputs": [],
  "commonOutputs": [],
  "relatedOperators": [],
  "workflowPatterns": [],
  "images": [],
  "videos": [],
  "assets": [],
  "keywords": [
    "face",
    "track",
    "chop",
    "only",
    "supported",
    "under",
    "microsoft",
    "windows",
    "operating",
    "system.hardware:",
    "uses",
    "augmented",
    "reality"
  ],
  "tags": [
    "CHOP",
    "TouchDesigner",
    "Face Track CHOP"
  ],
  "searchWeight": 1,
  "contentHash": "",
  "processingDate": "2025-08-07T07:50:00.597Z",
  "processingVersion": "1.0.0",
  "isValid": true,
  "validationErrors": []
}